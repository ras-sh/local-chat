// WebLLM with Qwen provides browser-based AI inference via WebGPU
// Uses Qwen 0.6B for fast, lightweight inference
export const MODEL_INFO = {
  name: "Qwen 0.6B (WebLLM)",
  description:
    "Uses Qwen 0.6B model running locally in your browser via WebGPU - fast, private, and works on all WebGPU-compatible browsers",
};
